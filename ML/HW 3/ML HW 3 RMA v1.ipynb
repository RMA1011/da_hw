{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import lightautoml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# ДЗ ML3:\n",
    "# 1. Датасет titanic, разделить на train/test в пропорции 0.7/0.3\n",
    "# 2. Предсказать возраст Age на основании других параметров (отберите сами какие)\n",
    "# 3. Подобрать оптимальные гиперпараметры модели с помощью GridSearch, в идеале получить лучшее качество на test\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Резюме\n",
    "\n",
    "# Результат линейной регрессии на тесте после оптимизации (MSE):  175.53050966534144\n",
    "# Лучшие параметры регрессии: {'fit_intercept': True, 'positive': False}\n",
    "\n",
    "# Лучший показатель DecisionTreeRegressor на тесте при оптимизации (MSE):  205.57545932291464\n",
    "# Лучшие параметры дерева: {'max_depth': 2, 'min_samples_leaf': 9, 'min_samples_split': 2}\n",
    "\n",
    "# Лучший показатель XGBRegressor на тесте при оптимизации (MSE):  201.8597682322448\n",
    "# Лучшие параметры:  {'colsample_bytree': 0.0, 'gamma': 0.0, 'learning_rate': 0.35, 'max_depth': 4, 'min_child_weight': 7}\n",
    "\n",
    "# Лучший показатель LAML на тесте при оптимизации (MSE):  196.89774507441547\n",
    "# Cпецификация\n",
    "# Final prediction for new objects (level 0) = \n",
    "#\t 0.66269 * (5 averaged models Lvl_0_Pipe_0_Mod_0_LinearL2) +\n",
    "#\t 0.33731 * (5 averaged models Lvl_0_Pipe_1_Mod_3_Tuned_CatBoost) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('https://raw.githubusercontent.com/RMA1011/da_hw/main/ML/HW%203/titanic.csv')\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Обработка данных\n",
    "raw_data.dropna(inplace = True)\n",
    "features = raw_data[['Survived','Pclass','Sex','SibSp','Fare']]\n",
    "features['Sex'].replace({'male':0,'female':1}, inplace = True)\n",
    "target = raw_data['Age']\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = 0.3, random_state = RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результат линейной регрессии на тесте до оптимизации (MSE):  175.53050966534144\n",
      "Результат линейной регрессии на обучении после оптимизации (MSE):  239.40141875015624\n",
      "Результат линейной регрессии на тесте после оптимизации (MSE):  175.53050966534144\n",
      "Лучшие параметры:  {'fit_intercept': True, 'positive': False}\n"
     ]
    }
   ],
   "source": [
    "#### Пробуем линейную регрессию\n",
    "lin_mod = LinearRegression()\n",
    "fit_mod = LinearRegression().fit(X = X_train,y = y_train)\n",
    "pred_lin = fit_mod.predict(X_test)\n",
    "print('Результат линейной регрессии на тесте до оптимизации (MSE): ', MSE(y_test, pred_lin))\n",
    "parameters = {'fit_intercept': [True,False],\n",
    "'positive':[True, False]}\n",
    "lin_grid = GridSearchCV(estimator = lin_mod, scoring = 'neg_mean_squared_error',param_grid = parameters, cv = 5, n_jobs =-1, refit = True)\n",
    "lin_grid = lin_grid.fit(X_train, y_train)\n",
    "best_lin_model = lin_grid.best_estimator_\n",
    "lin_pred = best_lin_model.predict(X_test)\n",
    "\n",
    "print('Результат линейной регрессии на обучении после оптимизации (MSE): ', -1*lin_grid.best_score_)\n",
    "print('Результат линейной регрессии на тесте после оптимизации (MSE): ', MSE(y_test, lin_pred))\n",
    "print('Лучшие параметры: ', lin_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучший показатель на обучении до оптимизации (MSE):  342.5092072727272\n",
      "Лучшие параметры:  {'max_depth': 2, 'min_samples_leaf': 9, 'min_samples_split': 2}\n",
      "Лучший показатель на обучении при оптимизации (MSE):  227.80952042290446\n",
      "Лучший показатель на тесте при оптимизации (MSE):  205.57545932291464\n"
     ]
    }
   ],
   "source": [
    "#### Пробуем дерево\n",
    "decis_tree = DecisionTreeRegressor(random_state=42)\n",
    "parameters_tree = {\n",
    "    'max_depth':  [1,2,3,4,5,6,7,8,9,10],\n",
    "    'min_samples_split': [2,3,4,5,6,7,8,9,10],\n",
    "    'min_samples_leaf': [2,3,4,5,6,7,8,9,10]\n",
    "}\n",
    "\n",
    "fit_tree = decis_tree.fit(X_train, y_train)\n",
    "fit_tree_pred = fit_tree.predict(X_test)\n",
    "print('Лучший показатель на обучении до оптимизации (MSE): ', MSE(y_test, fit_tree_pred))\n",
    "\n",
    "decis_grid = GridSearchCV(\n",
    "    estimator = decis_tree, \n",
    "    param_grid = parameters_tree,\n",
    "    scoring = 'neg_mean_squared_error',\n",
    "    cv = 5,\n",
    "    refit = True     \n",
    ")\n",
    "\n",
    "decis_grid = decis_grid.fit(X_train, y_train)\n",
    "print('Лучшие параметры: ',decis_grid.best_params_)\n",
    "print('Лучший показатель на обучении при оптимизации (MSE): ', -1*decis_grid.best_score_)\n",
    "best_decis_model = decis_grid.best_estimator_\n",
    "best_decis_model = best_decis_model.fit(X_train,y_train)\n",
    "best_predicted = best_decis_model.predict(X_test)\n",
    "print('Лучший показатель на тесте при оптимизации (MSE): ', MSE(y_test, best_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результат на тесте до оптимизации (MSE) :  208.103283\n",
      "Лучшие параметры:  {'colsample_bytree': 0.0, 'gamma': 0.0, 'learning_rate': 0.35, 'max_depth': 4, 'min_child_weight': 7}\n",
      "Лучший показатель на обучении при оптимизации (MSE):  229.63715166508436\n",
      "Лучший показатель на тесте при оптимизации (MSE):  201.8597682322448\n"
     ]
    }
   ],
   "source": [
    "### Пробуем xgboost!\n",
    "\n",
    "xgb_r = xgb.XGBRegressor(objective ='reg:squarederror',\n",
    "                  n_estimators = 10, seed = RANDOM_STATE)\n",
    "\n",
    "xgb_r.fit(X_train, y_train)\n",
    "xgb_pred = xgb_r.predict(X_test)\n",
    "print(\"Результат на тесте до оптимизации (MSE) : % f\" %(MSE(y_test, xgb_pred)))\n",
    "\n",
    "parameters_xgb = [\n",
    "    {\n",
    "    'learning_rate': (0.01,0.10,0.2,0.35,0.5),\n",
    "    'max_depth': [3,4,5,6,7],\n",
    "    'min_child_weight': [1,2,3,4,5,6,7],\n",
    "    'gamma': [0.0,0.1,0.2,0.3,0.4,0.5],\n",
    "    'colsample_bytree': [0.0,0.1,0.2,0.3,0.4,0.5]\n",
    "    }\n",
    "]\n",
    "\n",
    "xgb_grid = GridSearchCV(\n",
    "    estimator = xgb_r, \n",
    "    param_grid = parameters_xgb,\n",
    "    scoring = 'neg_mean_squared_error',\n",
    "    cv = 5,\n",
    "    refit = True)\n",
    "\n",
    "xgb_grid = xgb_grid.fit(X_train, y_train)\n",
    "print('Лучшие параметры: ',xgb_grid.best_params_)\n",
    "print('Лучший показатель на обучении при оптимизации (MSE): ', -1*xgb_grid.best_score_)\n",
    "best_xgb_model = xgb_grid.best_estimator_\n",
    "best_xgb_model = best_xgb_model.fit(X_train,y_train)\n",
    "xgb_opt_pred = best_xgb_model.predict(X_test)\n",
    "print('Лучший показатель на тесте при оптимизации (MSE): ', MSE(y_test, xgb_opt_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:02:12] Stdout logging level is INFO.\n",
      "[08:02:12] Task: reg\n",
      "\n",
      "[08:02:12] Start automl preset with listed constraints:\n",
      "[08:02:12] - time: 3600.00 seconds\n",
      "[08:02:12] - CPU: 4 cores\n",
      "[08:02:12] - memory: 16 GB\n",
      "\n",
      "[08:02:12] \u001b[1mTrain data shape: (128, 6)\u001b[0m\n",
      "\n",
      "[08:02:19] Layer \u001b[1m1\u001b[0m train process start. Time left 3593.45 secs\n",
      "[08:02:19] Start fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m ...\n",
      "[08:02:21] Fitting \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m finished. score = \u001b[1m-205.84605895582203\u001b[0m\n",
      "[08:02:21] \u001b[1mLvl_0_Pipe_0_Mod_0_LinearL2\u001b[0m fitting and predicting completed\n",
      "[08:02:21] Time left 3590.78 secs\n",
      "\n",
      "[08:02:22] \u001b[1mSelector_LightGBM\u001b[0m fitting and predicting completed\n",
      "[08:02:22] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m ...\n",
      "[08:02:22] Fitting \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m finished. score = \u001b[1m-217.48509883350934\u001b[0m\n",
      "[08:02:22] \u001b[1mLvl_0_Pipe_1_Mod_0_LightGBM\u001b[0m fitting and predicting completed\n",
      "[08:02:22] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ... Time budget is 300.00 secs\n",
      "[08:02:37] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m completed\n",
      "[08:02:37] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m ...\n",
      "[08:02:37] Fitting \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m finished. score = \u001b[1m-218.19963916340126\u001b[0m\n",
      "[08:02:37] \u001b[1mLvl_0_Pipe_1_Mod_1_Tuned_LightGBM\u001b[0m fitting and predicting completed\n",
      "[08:02:37] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m ...\n",
      "[08:02:38] Fitting \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m finished. score = \u001b[1m-222.46891062473068\u001b[0m\n",
      "[08:02:38] \u001b[1mLvl_0_Pipe_1_Mod_2_CatBoost\u001b[0m fitting and predicting completed\n",
      "[08:02:38] Start hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ... Time budget is 300.00 secs\n",
      "[08:02:50] Hyperparameters optimization for \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m completed\n",
      "[08:02:50] Start fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m ...\n",
      "[08:02:50] Fitting \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m finished. score = \u001b[1m-216.9066794179009\u001b[0m\n",
      "[08:02:50] \u001b[1mLvl_0_Pipe_1_Mod_3_Tuned_CatBoost\u001b[0m fitting and predicting completed\n",
      "[08:02:50] Time left 3562.02 secs\n",
      "\n",
      "[08:02:50] \u001b[1mLayer 1 training completed.\u001b[0m\n",
      "\n",
      "[08:02:50] Blending: optimization starts with equal weights and score \u001b[1m-209.34134750154658\u001b[0m\n",
      "[08:02:50] Blending: iteration \u001b[1m0\u001b[0m: score = \u001b[1m-202.45706988022266\u001b[0m, weights = \u001b[1m[0.6287758  0.06193072 0.         0.09162311 0.21767038]\u001b[0m\n",
      "[08:02:50] Blending: iteration \u001b[1m1\u001b[0m: score = \u001b[1m-201.97869815916656\u001b[0m, weights = \u001b[1m[0.66269696 0.         0.         0.         0.33730304]\u001b[0m\n",
      "[08:02:50] Blending: iteration \u001b[1m2\u001b[0m: score = \u001b[1m-201.9786961631473\u001b[0m, weights = \u001b[1m[0.66269267 0.         0.         0.         0.33730733]\u001b[0m\n",
      "[08:02:50] Blending: iteration \u001b[1m3\u001b[0m: score = \u001b[1m-201.9786961631473\u001b[0m, weights = \u001b[1m[0.66269267 0.         0.         0.         0.33730733]\u001b[0m\n",
      "[08:02:50] Blending: no score update. Terminated\n",
      "\n",
      "[08:02:50] \u001b[1mAutoml preset training completed in 38.09 seconds\u001b[0m\n",
      "\n",
      "[08:02:50] Model description:\n",
      "Final prediction for new objects (level 0) = \n",
      "\t 0.66269 * (5 averaged models Lvl_0_Pipe_0_Mod_0_LinearL2) +\n",
      "\t 0.33731 * (5 averaged models Lvl_0_Pipe_1_Mod_3_Tuned_CatBoost) \n",
      "\n",
      "Результат LAML на обучении:  201.97869524213266\n",
      "Результат LAML на тесте:  196.89774507441547\n"
     ]
    }
   ],
   "source": [
    "### Пробуем LightAutoML\n",
    "### пример: https://colab.research.google.com/github/AILab-MLTools/LightAutoML/blob/master/examples/tutorials/Tutorial_1_basics.ipynb#scrollTo=yl0IidgAD1_e\n",
    "### https://colab.research.google.com/github/AILab-MLTools/LightAutoML/blob/master/examples/tutorials/Tutorial_2_WhiteBox_AutoWoE.ipynb\n",
    "\n",
    "from lightautoml.automl.presets.tabular_presets import TabularAutoML, TabularUtilizedAutoML\n",
    "from lightautoml.tasks import Task\n",
    "from lightautoml.report.report_deco import ReportDeco\n",
    "\n",
    "roles = {'target': 'Age'}\n",
    "task = Task(name = 'reg', metric = 'mse')\n",
    "data_automl = raw_data[['Age','Survived','Pclass','Sex','SibSp','Fare']]\n",
    "data_automl['Sex'].replace({'male':0,'female':1}, inplace = True)\n",
    "\n",
    "automl_model = TabularAutoML(\n",
    "task = task, reader_params= {'random_state': RANDOM_STATE, 'cv':5}\n",
    ")\n",
    "\n",
    "tr_data_automl, te_data_automl = train_test_split(\n",
    "    data_automl,\n",
    "    test_size=0.3, # из технического задания\n",
    "    #stratify= data_automl['Age'],\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "oof_pred_automl = automl_model.fit_predict(tr_data_automl, roles = roles, verbose = 1)\n",
    "te_pred_automl = automl_model.predict(te_data_automl)\n",
    "print(f'Результат LAML на обучении: ',MSE(tr_data_automl['Age'].values, oof_pred_automl.data[:, 0]))\n",
    "print(f'Результат LAML на тесте: ',MSE(te_data_automl['Age'].values, te_pred_automl.data[:, 0]))\n",
    "print(automl_model.create_model_str_desc())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final prediction for new objects (level 0) = \n",
      "\t 0.66269 * (5 averaged models Lvl_0_Pipe_0_Mod_0_LinearL2) +\n",
      "\t 0.33731 * (5 averaged models Lvl_0_Pipe_1_Mod_3_Tuned_CatBoost) \n"
     ]
    }
   ],
   "source": [
    "#print(automl_model.create_model_str_desc())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1835615888dd5962b87b68afec97ab70ea34dcf06a519d9b229643f1246f266c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
